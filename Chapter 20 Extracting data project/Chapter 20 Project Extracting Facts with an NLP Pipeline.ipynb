{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Extracting Entities from Text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the large English NLP model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text we want to examine\n",
    "text = \"\"\"London is the capital and most populous city of England and\n",
    "the United Kingdom. Standing on the River Thames in the south east\n",
    "of the island of Great Britain, London has been a major settlement for\n",
    "two millennia. It was founded by the Romans, who named it Londinium\"\"\"\n",
    "\n",
    "# Parse the text with spaCy. This runs the entire NLP pipeline.\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "London (GPE)\n",
      "England (GPE)\n",
      "the United Kingdom (GPE)\n",
      "the River Thames (LOC)\n",
      "Great Britain (GPE)\n",
      "London (GPE)\n",
      "two millennia (DATE)\n",
      "Romans (NORP)\n",
      "Londinium (LOC)\n"
     ]
    }
   ],
   "source": [
    "# For example, this will print out all the named entities that were detected:\n",
    "for entity in doc.ents:\n",
    "    print(f\"{entity.text} ({entity.label_})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Building a data scrubber***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace a token with \"REDACTED\" if it is a name\n",
    "def replace_name_with_placeholder(token):\n",
    "    if token.ent_iob != 0 and token.ent_type_ == \"PERSON\":\n",
    "        return \"[REDACTED]\"\n",
    "    else:\n",
    "        return token.string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all the entities in a document and check if they are names\n",
    "def scrub(text):\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "        ent.merge()\n",
    "    tokens = map(replace_name_with_placeholder, doc)\n",
    "    return \"\".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "In 1950, [REDACTED]published his famous article \"Computing Machinery\n",
      "and Intelligence\". In 1957, Noam 'Chomskys Syntactic Structures\n",
      "revolutionized Linguistics with \"universal grammar\", a rule based system\n",
      "of syntactic structures.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"\n",
    "In 1950, Alan Turing published his famous article \"Computing Machinery\n",
    "and Intelligence\". In 1957, Noam 'Chomskys Syntactic Structures\n",
    "revolutionized Linguistics with \"universal grammar\", a rule based system\n",
    "of syntactic structures.\n",
    "\"\"\"\n",
    "\n",
    "print(scrub(s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Extracting Facts from Text***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.extract\n",
    "from pathlib import Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The text we want to examine\n",
    "text = Path(\"london.txt\").read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the document with spaCy\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract semi-structured statements \n",
    "statements = textacy.extract.semistructured_statements(doc, \"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the things I know about London:\n",
      " - the capital and most populous city of England and the United Kingdom.\n",
      "\n",
      " - a major settlement for two millennia\n",
      " - the world's most populous city from around 1831 to 1925\n",
      " - beyond all comparison the largest\n",
      "town in England\n",
      " - still very compact\n",
      " - the world's largest city from about 1831 to 1925\n",
      " - the seat of the\n",
      "Government of the United Kingdom\n",
      " - vulnerable to flooding.\n",
      "\n",
      " - \"one of the World's\n",
      "Greenest Cities\" with more than 40 percent green space or open water\n",
      " - the most\n",
      "populous city and metropolitan area of the European Union and the second most\n",
      "populous in Europe\n",
      " - the 19th largest city and the 18th largest\n",
      "metropolitan region in the world\n",
      " - Christian, and has a large number of churches, particularly\n",
      "in the City of London\n",
      " - also home to sizeable Muslim, Hindu, Sikh, and Jewish\n",
      "communities\n",
      " - also home to 42\n",
      "Hindu temples\n",
      " - one of the pre-eminent financial centres of the world as the most\n",
      "important location for international finance\n",
      " - the world top city\n",
      "destination as ranked by TripAdvisor users\n",
      " - a major\n",
      "international air transport hub with the busiest city airspace in the world.\n",
      "\n",
      " - the centre of the National Rail network, with 70 percent of rail journeys\n",
      "starting or ending in London\n",
      " - home to five major medical schools â€\n",
      " - home to designers Vivienne Westwood, Galliano, Stella\n",
      "McCartney, Manolo Blahnik, and Jimmy Choo, among others\n",
      " - the setting for many works of\n",
      "literature\n",
      " - also a centre for urban music\n",
      " - the \"greenest city\" in Europe with 35,000 acres\n",
      "of public parks, woodlands and gardens\n"
     ]
    }
   ],
   "source": [
    "# Print the results\n",
    "print(\"Here are the things I know about London:\")\n",
    "\n",
    "for statement in statements:\n",
    "    subject, verb, fact = statement\n",
    "    print(f\" - {fact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Extracting Noun Chunks***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import textacy.extract\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the large English NLP model\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# The text we want to examine\n",
    "text = Path(\"london.txt\").read_text()\n",
    "\n",
    "# Parse the document with spaCy\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract semi-structured statements\n",
    "noun_chunks = textacy.extract.noun_chunks(doc, min_freq=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "major centre\n",
      "\n",
      "london underground\n",
      "large number\n",
      "european union\n",
      "population density\n",
      "greater london\n",
      "\n",
      "london school\n",
      "major centre\n",
      "\n",
      "population density\n",
      "greater london's population\n",
      "united kingdom\n",
      "royal albert hall\n",
      "2011 census\n",
      "second world war\n",
      "canary wharf\n",
      "river thames\n",
      "westminster abbey\n",
      "west london\n",
      "epping forest\n",
      "london school\n",
      "london eye\n",
      "other city\n",
      "new york city\n",
      "greater london authority\n",
      "central london\n",
      "national gallery\n",
      "regent's park\n",
      "hampstead heath\n",
      "tate modern\n",
      "national statistics\n",
      "\n",
      "eight royal parks\n",
      "eight royal parks\n",
      "royal opera house\n",
      "\n",
      "city centre\n",
      "london underground\n",
      "inner london\n",
      "east end\n",
      "london's population\n",
      "british museum\n",
      "office space\n",
      "great fire\n",
      "\n",
      "other city\n",
      "city centre\n",
      "\n",
      "national gallery\n",
      "outer london\n",
      "south london\n",
      "trafalgar square\n"
     ]
    }
   ],
   "source": [
    "# Convert noun chunks to lowercase strings\n",
    "noun_chunks = map(str, noun_chunks)\n",
    "noun_chunks = map(str.lower, noun_chunks)\n",
    "\n",
    "# Print out any nouns that are at least 2 words long\n",
    "for noun_chunk in set(noun_chunks):\n",
    "    if len(noun_chunk.split(\" \")) > 1:\n",
    "        print(noun_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
